{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations for code readability\n",
    "import typing as tp\n",
    "import chex\n",
    "\n",
    "# data read and split\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# nn\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from flax import nnx\n",
    "from flax.typing import Array\n",
    "import optax\n",
    "\n",
    "from utils import (\n",
    "    MLP,\n",
    "    MAE,\n",
    "    _init_model,\n",
    "    batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MAE = \\frac{1}{n}\\sum_{i=0}^{n}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=0}^{n}(y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"./data/BostonHousing.csv\")\n",
    "\n",
    "X = df.drop(pl.col(\"medv\")).to_jax()\n",
    "y = df.select(pl.col(\"medv\")).to_jax().squeeze()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.195,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "X_train -= X_train.mean(axis=0)\n",
    "X_train /= X_train.std(axis=0)\n",
    "\n",
    "X_test -= X_test.mean(axis=0)\n",
    "X_test /= X_test.std(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  nn=Sequential(\n",
       "    layers=[Linear(\n",
       "      kernel=Param(\n",
       "        value=Array(shape=(13, 5), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array(shape=(5,), dtype=float32)\n",
       "      ),\n",
       "      in_features=13,\n",
       "      out_features=5,\n",
       "      use_bias=True,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      precision=None,\n",
       "      kernel_init=<function variance_scaling.<locals>.init at 0x0000028FF2C9ADC0>,\n",
       "      bias_init=<function zeros at 0x0000028FE21D3EE0>,\n",
       "      dot_general=<function dot_general at 0x0000028FE1C37550>\n",
       "    ), <PjitFunction of <function silu at 0x0000028FE21E95E0>>, Linear(\n",
       "      kernel=Param(\n",
       "        value=Array(shape=(5, 1), dtype=float32)\n",
       "      ),\n",
       "      bias=Param(\n",
       "        value=Array([0.], dtype=float32)\n",
       "      ),\n",
       "      in_features=5,\n",
       "      out_features=1,\n",
       "      use_bias=True,\n",
       "      dtype=None,\n",
       "      param_dtype=<class 'jax.numpy.float32'>,\n",
       "      precision=None,\n",
       "      kernel_init=<function variance_scaling.<locals>.init at 0x0000028FF2C9ADC0>,\n",
       "      bias_init=<function zeros at 0x0000028FE21D3EE0>,\n",
       "      dot_general=<function dot_general at 0x0000028FE1C37550>\n",
       "    )]\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(input_dim=13, hidden=(5,), output_dim=1, rngs=nnx.Rngs(42))\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.1617936], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model(jnp.ones(13))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model: MLP, \n",
    "            features: Array, \n",
    "            targets: Array) -> tuple[Array]:\n",
    "    \"\"\"Computes L2-Loss with optax. Returns loss and logits\"\"\"\n",
    "    logits = model(features)\n",
    "    loss = jnp.mean(\n",
    "        optax.l2_loss(\n",
    "            predictions=logits.squeeze(), targets=targets\n",
    "        )\n",
    "    )\n",
    "    return loss, logits\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(graphdef: nnx.GraphDef, \n",
    "               state: nnx.GraphState,   \n",
    "               features: Array, \n",
    "               targets: Array) -> nnx.GraphState:\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    model, optimizer, metrics = nnx.merge(graphdef, state)\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, features, targets)\n",
    "    metrics.update(loss=loss, logits=logits, targets=targets)\n",
    "    optimizer.update(grads)\n",
    "    _, state = nnx.split((model, optimizer, metrics))\n",
    "    return state\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(graphdef: nnx.GraphDef, \n",
    "              state: nnx.GraphState,   \n",
    "              features: Array,\n",
    "              targets: Array) -> nnx.GraphState:\n",
    "    \"\"\"Eval for single step\"\"\"\n",
    "    model, optimizer, metrics = nnx.merge(graphdef, state)\n",
    "    loss, logits = loss_fn(model, features, targets)\n",
    "    metrics.update(loss=loss, logits=logits, targets=targets)\n",
    "    _, state = nnx.split((model, optimizer, metrics))\n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_metrics_hisory() -> dict[str, Array]:\n",
    "    \"\"\"Creates metrics_history\n",
    "    Example:\n",
    "    >>> metrics = _init_metrics_hisory()\n",
    "    >>> metrics\n",
    "    {'train_loss': [], 'train_mae': [], 'test_loss': [], 'test_mae': []}\n",
    "    \"\"\"\n",
    "    metrics_history = {\n",
    "        'train_loss': [],\n",
    "        'train_mae': [],\n",
    "        'test_loss': [],\n",
    "        'test_mae': [],\n",
    "    }\n",
    "    return metrics_history\n",
    "\n",
    "\n",
    "def run(X_train: Array,\n",
    "        y_train: Array,\n",
    "        X_test: Array,\n",
    "        y_test: Array,\n",
    "        batch_size: int = 8,\n",
    "        key: tp.Optional[chex.PRNGKey] = None,\n",
    "        input_dim: int = 60,\n",
    "        hidden: tp.Optional[tuple[int]] = None,\n",
    "        output_dim: int = 1,\n",
    "        k: int = 3,\n",
    "        num_epochs: int = 10) -> tuple[MLP, dict[str, Array], nnx.State, nnx.State]:\n",
    "    \"\"\"Inits model and it's components and run train and test\"\"\"\n",
    "    if key is None:\n",
    "        key = jax.random.key(42)\n",
    "\n",
    "    batch_key, model_key = jax.random.split(key,)\n",
    "    model_key = nnx.Rngs(model_key)\n",
    "    \n",
    "    # get model and default weights of the first layer\n",
    "    num_val_samples = len(X_train) // k\n",
    "    metrics_history = _init_metrics_hisory()\n",
    "    metrics = nnx.MultiMetric(\n",
    "        mae=MAE(),\n",
    "        loss=nnx.metrics.Average('loss'),\n",
    "    )\n",
    "    test_metrics = {\n",
    "        'test_loss': [],\n",
    "        'test_mae': [],\n",
    "    }\n",
    "    for k_fold in range(k):\n",
    "        model, _ = _init_model(input_dim=input_dim, hidden=hidden, output_dim=output_dim, rngs=model_key)\n",
    "        optimizer = nnx.Optimizer(model, optax.rmsprop(learning_rate=3e-3))\n",
    "        print(\"\\nK-fold:\", k_fold)\n",
    "        X_val = X_train[k_fold * num_val_samples: (k_fold + 1) * num_val_samples]\n",
    "        y_val = y_train[k_fold * num_val_samples: (k_fold + 1) * num_val_samples]\n",
    "\n",
    "        partial_train_data = jnp.concatenate(\n",
    "            [X_train[:k_fold * num_val_samples], \n",
    "             X_train[(k_fold + 1) * num_val_samples:]], axis=0)\n",
    "        partial_train_targets = jnp.concatenate(\n",
    "            [y_train[:k_fold * num_val_samples], \n",
    "             y_train[(k_fold + 1) * num_val_samples:]], axis=0)\n",
    "        for _ in range(num_epochs):\n",
    "            # train\n",
    "            model.train()\n",
    "            graphdef, state = nnx.split((model, optimizer, metrics))\n",
    "\n",
    "            # new order for shuffle\n",
    "            batch_key = jax.random.split(batch_key)[0]\n",
    "            for X_batched, y_batched in batch(X=partial_train_data, \n",
    "                                              y=partial_train_targets,\n",
    "                                              batch_size=batch_size,\n",
    "                                              key=batch_key,\n",
    "                                              train=True):\n",
    "                state = train_step(graphdef=graphdef, \n",
    "                                   state=state,\n",
    "                                   features=X_batched, \n",
    "                                   targets=y_batched)\n",
    "\n",
    "            # if flax >= 0.10.0\n",
    "            # nnx.update((model, optimizer, metrics), state)\n",
    "            \n",
    "            # else\n",
    "            model, optimizer, metrics = nnx.merge(graphdef, state)\n",
    "            # store train metrics\n",
    "            for metric, value in metrics.compute().items():     \n",
    "                metrics_history[f'train_{metric}'].append(value)\n",
    "            metrics.reset()\n",
    "\n",
    "            # eval\n",
    "            model.eval()\n",
    "            graphdef, state = nnx.split((model, optimizer, metrics))\n",
    "            for X_batched, y_batched in batch(X=X_val, \n",
    "                                              y=y_val,\n",
    "                                              batch_size=batch_size,\n",
    "                                              key=batch_key,\n",
    "                                              train=False):\n",
    "                state = eval_step(graphdef=graphdef,\n",
    "                                  state=state, \n",
    "                                  features=X_batched, \n",
    "                                  targets=y_batched)\n",
    "            \n",
    "            # if flax >= 0.10.0\n",
    "            # nnx.update((model, optimizer, metrics), state)\n",
    "            \n",
    "            # else\n",
    "            model, optimizer, metrics = nnx.merge(graphdef, state)\n",
    "            # store eval metrics\n",
    "            for metric, value in metrics.compute().items():    \n",
    "                metrics_history[f'test_{metric}'].append(value)\n",
    "            metrics.reset() \n",
    "\n",
    "        model.eval()\n",
    "        graphdef, state = nnx.split((model, optimizer, metrics))\n",
    "        for X_batched, y_batched in batch(X=X_val, \n",
    "                                          y=y_val,\n",
    "                                          batch_size=batch_size,\n",
    "                                          key=batch_key,\n",
    "                                          train=False):\n",
    "            state = eval_step(graphdef=graphdef,\n",
    "                              state=state,\n",
    "                              features=X_batched, \n",
    "                              targets=y_batched)\n",
    "        \n",
    "        # store eval metrics\n",
    "        print(\"Test\")\n",
    "\n",
    "        # if flax >= 0.10.0\n",
    "        # nnx.update((model, optimizer, metrics), state)\n",
    "        \n",
    "        # else\n",
    "        model, _, metrics = nnx.merge(graphdef, state)\n",
    "        for metric, value in metrics.compute().items():    \n",
    "            test_metrics[f'test_{metric}'].append(value)\n",
    "            print(f\"{metric}: {value}\")\n",
    "        metrics.reset() \n",
    "\n",
    "    \n",
    "    return model, metrics_history, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K-fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Артём\\Desktop\\лекции\\DIIS\\labs\\utils.py:238: UserWarning: Can't split X with 101 examples in equal batches with batch_size=9. Last batch will be droped\n",
      "  warnings.warn(f\"Can't split X with {X.shape[0]} examples in equal batches with {batch_size=}. Last batch will be droped\", category=UserWarning)\n",
      "c:\\Users\\Артём\\Desktop\\лекции\\DIIS\\labs\\utils.py:238: UserWarning: Can't split X with 101 examples in equal batches with batch_size=9. Last batch will be droped\n",
      "  warnings.warn(f\"Can't split X with {X.shape[0]} examples in equal batches with {batch_size=}. Last batch will be droped\", category=UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "mae: 8.474502563476562\n",
      "loss: 5.22266149520874\n",
      "\n",
      "K-fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Артём\\Desktop\\лекции\\DIIS\\labs\\utils.py:238: UserWarning: Can't split X with 101 examples in equal batches with batch_size=9. Last batch will be droped\n",
      "  warnings.warn(f\"Can't split X with {X.shape[0]} examples in equal batches with {batch_size=}. Last batch will be droped\", category=UserWarning)\n",
      "c:\\Users\\Артём\\Desktop\\лекции\\DIIS\\labs\\utils.py:238: UserWarning: Can't split X with 101 examples in equal batches with batch_size=9. Last batch will be droped\n",
      "  warnings.warn(f\"Can't split X with {X.shape[0]} examples in equal batches with {batch_size=}. Last batch will be droped\", category=UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "mae: 9.651590347290039\n",
      "loss: 9.30495548248291\n",
      "\n",
      "K-fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Артём\\Desktop\\лекции\\DIIS\\labs\\utils.py:238: UserWarning: Can't split X with 101 examples in equal batches with batch_size=9. Last batch will be droped\n",
      "  warnings.warn(f\"Can't split X with {X.shape[0]} examples in equal batches with {batch_size=}. Last batch will be droped\", category=UserWarning)\n",
      "c:\\Users\\Артём\\Desktop\\лекции\\DIIS\\labs\\utils.py:238: UserWarning: Can't split X with 101 examples in equal batches with batch_size=9. Last batch will be droped\n",
      "  warnings.warn(f\"Can't split X with {X.shape[0]} examples in equal batches with {batch_size=}. Last batch will be droped\", category=UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "mae: 10.561206817626953\n",
      "loss: 4.997997760772705\n",
      "\n",
      "K-fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Артём\\Desktop\\лекции\\DIIS\\labs\\utils.py:238: UserWarning: Can't split X with 101 examples in equal batches with batch_size=9. Last batch will be droped\n",
      "  warnings.warn(f\"Can't split X with {X.shape[0]} examples in equal batches with {batch_size=}. Last batch will be droped\", category=UserWarning)\n",
      "c:\\Users\\Артём\\Desktop\\лекции\\DIIS\\labs\\utils.py:238: UserWarning: Can't split X with 101 examples in equal batches with batch_size=9. Last batch will be droped\n",
      "  warnings.warn(f\"Can't split X with {X.shape[0]} examples in equal batches with {batch_size=}. Last batch will be droped\", category=UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "mae: 8.21519947052002\n",
      "loss: 5.934994220733643\n"
     ]
    }
   ],
   "source": [
    "_, metrics_history, test_metrics = run(X_train=X_train, \n",
    "                                       y_train=y_train, \n",
    "                                       X_test=X_test, \n",
    "                                       y_test=y_test,\n",
    "                                       batch_size=9,\n",
    "                                       key=jax.random.key(42),\n",
    "                                       input_dim=X_train.shape[1], \n",
    "                                       hidden=(64,), \n",
    "                                       output_dim=1, \n",
    "                                       num_epochs=100, \n",
    "                                       k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': [Array(5.2226615, dtype=float32),\n",
       "  Array(9.3049555, dtype=float32),\n",
       "  Array(4.9979978, dtype=float32),\n",
       "  Array(5.934994, dtype=float32)],\n",
       " 'test_mae': [Array(8.474503, dtype=float32),\n",
       "  Array(9.65159, dtype=float32),\n",
       "  Array(10.561207, dtype=float32),\n",
       "  Array(8.215199, dtype=float32)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
